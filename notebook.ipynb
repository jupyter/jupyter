{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classifier: TF-IDF + Logistic Regression\n",
    "This notebook trains a TF-IDF + Logistic Regression baseline on the IMDB dataset, performs GridSearchCV, inspects top features, and includes an optional DistilBERT fine-tune (GPU recommended)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install required packages (Colab or local)\n",
    "!pip install -q datasets scikit-learn transformers torch sentencepiece matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load IMDB dataset using Hugging Face datasets\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_ds = dataset[\"train\"].shuffle(seed=42)\n",
    "test_ds = dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# Optional: reduce size for quick runs (uncomment to use)\n",
    "# train_ds = train_ds.select(range(5000))\n",
    "# test_ds  = test_ds.select(range(2000))\n",
    "\n",
    "df_train = pd.DataFrame({\"text\": train_ds[\"text\"], \"label\": train_ds[\"label\"]})\n",
    "df_test  = pd.DataFrame({\"text\": test_ds[\"text\"],  \"label\": test_ds[\"label\"]})\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_train[\"text\"], df_train[\"label\"], test_size=0.2, random_state=42, stratify=df_train[\"label\"]\n",
    ")\n",
    "print(\"Training examples:\", len(X_train))\n",
    "print(\"Validation examples:\", len(X_val))\n",
    "print(\"Test examples:\", len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Baseline pipeline: TF-IDF + Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        strip_accents='unicode',\n",
    "        stop_words='english',\n",
    "        ngram_range=(1,2),\n",
    "        min_df=5,\n",
    "        max_df=0.9\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(solver=\"saga\", penalty=\"l2\", max_iter=2000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Validation confusion matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV (short grid for speed)\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__min_df\": [2,5],\n",
    "    \"tfidf__max_df\": [0.85, 0.95],\n",
    "    \"clf__C\": [0.1, 1.0]\n",
    "}\n",
    "gs = GridSearchCV(pipeline, param_grid, cv=3, scoring=\"f1\", n_jobs=-1, verbose=2)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV F1 score:\", gs.best_score_)\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "\n",
    "best_model = gs.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(\"Validation Accuracy (best):\", accuracy_score(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Final test evaluation\n",
    "y_test_pred = best_model.predict(df_test[\"text\"])\n",
    "print(\"Test Accuracy:\", accuracy_score(df_test[\"label\"], y_test_pred))\n",
    "print(classification_report(df_test[\"label\"], y_test_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect top features per class\n",
    "vec = best_model.named_steps[\"tfidf\"]\n",
    "clf = best_model.named_steps[\"clf\"]\n",
    "feature_names = np.array(vec.get_feature_names_out())\n",
    "topn = 25\n",
    "coef = clf.coef_[0]\n",
    "top_pos = np.argsort(coef)[-topn:][::-1]\n",
    "top_neg = np.argsort(coef)[:topn]\n",
    "print(\"Top positive tokens:\", \", \".join(feature_names[top_pos]))\n",
    "print(\"Top negative tokens:\", \", \".join(feature_names[top_neg]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: DistilBERT fine-tune (GPU recommended)\n",
    "Run this section only if you have GPU available. It fine-tunes DistilBERT on a subset for speed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    hf_train = dataset[\"train\"].shuffle(seed=42).select(range(8000)).map(tokenize, batched=True)\n",
    "    hf_test  = dataset[\"test\"].shuffle(seed=42).select(range(2000)).map(tokenize, batched=True)\n",
    "\n",
    "    hf_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]) \n",
    "    hf_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]) \n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./distilbert-sentiment\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        return {\"accuracy\": (preds == labels).mean()}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_test,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(eval_results)\n",
    "else:\n",
    "    print(\"GPU not available: skip DistilBERT fine-tune\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and suggestions\n",
    "- For quick iteration, reduce dataset sizes as shown above.\n",
    "- Tune TF-IDF and classifier hyperparameters based on validation performance.\n",
    "- Use cross-validation for robust estimates when dataset size permits.\n",
    "- DistilBERT training requires GPU for reasonable speed."
   ]
  }
 ]
}
