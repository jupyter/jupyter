{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Scholarship Synthesis\n",
    "\n",
    "**Tertiary Layer: Multi-Platform Research Synthesis**\n",
    "\n",
    "This notebook demonstrates integration of:\n",
    "- **Primary Layer**: Text atomization outputs (n-grams, entropy, glyphs)\n",
    "- **Secondary Layer**: AI-generated scholarship (Perplexity, Claude, GPT)\n",
    "- **Tertiary Layer**: Jupyter synthesis and visualization\n",
    "\n",
    "## Recursive Research Architecture\n",
    "\n",
    "```\n",
    "Literary Text → Atomization → JSON/Markdown\n",
    "                    ↓\n",
    "               AI Research (Perplexity → Claude → GPT)\n",
    "                    ↓\n",
    "               Jupyter Synthesis ← Pattern Analysis\n",
    "                    ↓\n",
    "               Iterative Refinement\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from atomization import TextAtomizer\n",
    "from scholarship import ScholarshipIngester, ScholarshipSynthesizer, MultiPlatformOrchestrator\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Atomization Data (Primary Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consolidated atomization archive\n",
    "atomization_archive = project_root / 'data' / 'processed' / 'atomization' / 'all_outputs.json'\n",
    "\n",
    "if atomization_archive.exists():\n",
    "    with open(atomization_archive, 'r', encoding='utf-8') as f:\n",
    "        atomization_data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(atomization_data)} atomization runs\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df_atomization = pd.DataFrame([\n",
    "        {\n",
    "            'date': item['date'],\n",
    "            'title': item['metadata']['title'],\n",
    "            'word_count': item['metadata']['word_count'],\n",
    "            'shannon_entropy': item['entropy']['shannon_entropy'],\n",
    "            'lexical_diversity': item['entropy']['lexical_diversity'],\n",
    "            'compression_ratio': item['entropy']['compression_ratio']\n",
    "        }\n",
    "        for item in atomization_data\n",
    "    ])\n",
    "    \n",
    "    display(df_atomization.head())\n",
    "else:\n",
    "    print(\"No atomization data found. Run 01_text_atomization_workflow.ipynb first.\")\n",
    "    df_atomization = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load AI Scholarship (Secondary Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scholarship_dir = project_root / 'output' / 'scholarship'\n",
    "\n",
    "# Initialize synthesizer\n",
    "synthesizer = ScholarshipSynthesizer(scholarship_dir)\n",
    "\n",
    "print(f\"Loaded {len(synthesizer.research_items)} research items\")\n",
    "\n",
    "if synthesizer.research_items:\n",
    "    # Convert to DataFrame\n",
    "    df_scholarship = pd.DataFrame([\n",
    "        {\n",
    "            'source': item.get('source'),\n",
    "            'title': item.get('title'),\n",
    "            'timestamp': item.get('timestamp', '')[:10],\n",
    "            'tags': ', '.join(item.get('tags', [])),\n",
    "            'citation_count': len(item.get('citations', []))\n",
    "        }\n",
    "        for item in synthesizer.research_items\n",
    "    ])\n",
    "    \n",
    "    display(df_scholarship.head())\n",
    "else:\n",
    "    print(\"\\nNo AI scholarship found yet.\")\n",
    "    print(\"\\nTo add scholarship, see example workflow below.\")\n",
    "    df_scholarship = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Layer Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_atomization is not None and len(df_atomization) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Entropy trends\n",
    "    axes[0, 0].plot(df_atomization['date'], df_atomization['shannon_entropy'], marker='o', color='steelblue')\n",
    "    axes[0, 0].set_title('Shannon Entropy Over Time')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Entropy')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Lexical diversity\n",
    "    axes[0, 1].plot(df_atomization['date'], df_atomization['lexical_diversity'], marker='s', color='coral')\n",
    "    axes[0, 1].set_title('Lexical Diversity Over Time')\n",
    "    axes[0, 1].set_xlabel('Date')\n",
    "    axes[0, 1].set_ylabel('Diversity Ratio')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Compression ratio\n",
    "    axes[1, 0].plot(df_atomization['date'], df_atomization['compression_ratio'], marker='^', color='seagreen')\n",
    "    axes[1, 0].set_title('Compression Ratio Over Time')\n",
    "    axes[1, 0].set_xlabel('Date')\n",
    "    axes[1, 0].set_ylabel('Ratio')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Word count distribution\n",
    "    axes[1, 1].hist(df_atomization['word_count'], bins=10, color='mediumpurple', edgecolor='black')\n",
    "    axes[1, 1].set_title('Word Count Distribution')\n",
    "    axes[1, 1].set_xlabel('Word Count')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient atomization data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Platform Research Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_scholarship is not None and len(df_scholarship) > 0:\n",
    "    # Research by platform\n",
    "    source_counts = df_scholarship['source'].value_counts()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Source distribution\n",
    "    source_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title('Research Items by AI Platform')\n",
    "    axes[0].set_xlabel('Platform')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Citation distribution\n",
    "    axes[1].hist(df_scholarship['citation_count'], bins=10, color='lightcoral', edgecolor='black')\n",
    "    axes[1].set_title('Citations per Research Item')\n",
    "    axes[1].set_xlabel('Number of Citations')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Frequent citations\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FREQUENTLY CITED SOURCES\")\n",
    "    print(\"=\" * 60)\n",
    "    frequent_cites = synthesizer.extract_citations(min_frequency=2)\n",
    "    for cite in frequent_cites[:5]:\n",
    "        print(f\"  [{cite.get('frequency')}x] {cite.get('title', 'Untitled')}\")\n",
    "else:\n",
    "    print(\"No scholarship data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Thematic Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_scholarship is not None and len(df_scholarship) > 0:\n",
    "    # Analyze by theme\n",
    "    theme_analysis = synthesizer.analyze_by_theme()\n",
    "    \n",
    "    print(\"RESEARCH THEMES\")\n",
    "    print(\"=\" * 60)\n",
    "    for theme, items in sorted(theme_analysis.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "        print(f\"\\n{theme.upper()} ({len(items)} items)\")\n",
    "        for item in items[:3]:  # Show top 3 per theme\n",
    "            print(f\"  - {item['title']} ({item['source'].title()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Synthesis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_scholarship is not None:\n",
    "    report = synthesizer.generate_synthesis_report()\n",
    "    print(report)\n",
    "    \n",
    "    # Save to output\n",
    "    report_path = project_root / 'output' / 'synthesis' / 'synthesis_report.md'\n",
    "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"\\nReport saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example: Add AI Scholarship (Secondary Layer)\n",
    "\n",
    "Demonstration of ingesting AI research outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarship import AISource\n",
    "\n",
    "# Initialize ingester\n",
    "ingester = ScholarshipIngester(scholarship_dir)\n",
    "\n",
    "# Example: Ingest Perplexity research\n",
    "perplexity_research = \"\"\"\n",
    "## Homeric Formulae and Oral Tradition\n",
    "\n",
    "Recent scholarship emphasizes the role of formulaic expressions in Homeric epic composition.\n",
    "Milman Parry's foundational work established that epithets like \"wine-dark sea\" and \n",
    "\"rosy-fingered dawn\" functioned as mnemonic devices for oral poets.\n",
    "\n",
    "Contemporary computational analysis confirms high repetition rates in the Odyssey,\n",
    "with n-gram studies revealing recursive patterns at multiple scales.\n",
    "\"\"\"\n",
    "\n",
    "# Ingest\n",
    "path = ingester.ingest_research(\n",
    "    content=perplexity_research,\n",
    "    source=AISource.PERPLEXITY,\n",
    "    query=\"Homeric formulae and computational text analysis\",\n",
    "    title=\"Formulaic Expression in Homer\",\n",
    "    tags=['homer', 'oral tradition', 'computational analysis', 'n-grams'],\n",
    "    citations=[\n",
    "        {'title': 'The Making of Homeric Verse', 'author': 'Milman Parry', 'url': 'https://example.com/parry'},\n",
    "        {'title': 'Digital Humanities and Classical Studies', 'url': 'https://example.com/dh-classics'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Ingested research saved to: {path}\")\n",
    "print(\"\\nRun this cell to add your own AI research outputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Multi-Platform Workflow Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize orchestrator\n",
    "orchestrator = MultiPlatformOrchestrator()\n",
    "\n",
    "# Generate workflow for literary analysis\n",
    "workflow_path = project_root / 'output' / 'scholarship' / 'workflow_literary_analysis.md'\n",
    "\n",
    "orchestrator.generate_workflow_file(\n",
    "    workflow_name='literary_analysis',\n",
    "    output_path=workflow_path,\n",
    "    params={\n",
    "        'topic': 'homecoming motifs',\n",
    "        'work': 'the Odyssey',\n",
    "        'comparison_works': 'Aeneid, Divine Comedy, Ulysses'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Workflow template generated: {workflow_path}\")\n",
    "print(\"\\nUse this template to structure multi-platform AI research workflows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: Recursive Iteration\n",
    "\n",
    "1. **Daily Atomization**: Continue processing literary excerpts → Primary Layer JSON/MD\n",
    "2. **AI Research Cycles**: Use workflows to orchestrate Perplexity → Claude → GPT\n",
    "3. **Pattern Mapping**: Track how atomization metrics correlate with scholarly themes\n",
    "4. **Recursive Refinement**: Feed synthesis insights back into atomization parameters\n",
    "\n",
    "### Breadth-and-Depth Method\n",
    "- **Breadth**: 61 days of identical *Odyssey* runs (baseline)\n",
    "- **Depth**: Test pits with *Metamorphoses*, *Canterbury Tales*, *Finnegans Wake*\n",
    "- **Remapping**: Synthesize cross-work patterns, adjust glyph fusion logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
